---
title: "ridgereg"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ridgereg}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, echo=FALSE}
library(LinearRegPackage)
library(MASS)
library(caret)
library(mlbench)
library(elasticnet)
```

# A simple prediction problem using our ridgereg() function.

This vignette demonstrates how to perform a simple prediction using our own ridgereg() function available in this package. 
There are multiple steps in this process:

### Step 1: Dividing the BostonHousing data into training and test sets using caret package.
We use the createDataPartition function from caret package to divide the data set into training and test sets.
```{r}
set.seed(42)
data("BostonHousing")
training_index <- caret::createDataPartition(BostonHousing$medv, p=0.77, list = F)
data_train <- BostonHousing[training_index,]
data_test <- BostonHousing[-training_index,]

```

### Step 2: Linear Regression and Linear Regression with forward selection of covariates.

#### Step 2.1: Simple Linear Regression:
We fit a simple linear regression method 'lm' to predict values for the test data set.
```{r}
lmFit <- caret::train(medv ~ .,
               data = data_train,
               method = 'lm',
               preProcess = c("scale", "center"))

bh_pred <- predict(lmFit, data_test)
```

We are predicting the Median Value of houses, by fitting the trained lmFit model on test data set and we use postResample to obtain RMSE, R-Squared and MAE values.
```{r, echo=TRUE,eval=TRUE}

untuned_perf <- caret::postResample(pred = bh_pred, obs = data_test$medv)
untuned_perf
```

#### Step 2.2: Linear Regression by forward selection of covariates:
We fit a linear regression with forward selection of covariates by method 'leapForward'
```{r}

lmFit_forward <- caret::train(medv ~ .,
               data = data_train,
               method = 'leapForward',
               preProcess = c("scale", "center"),
               tuneGrid = expand.grid(nvmax = seq(1, 13, 2))
               )
bh_pred_forward <- predict(lmFit_forward, data_test)
```

The results of forward selection of covariates and corresponding RMSE values are as shown below:
```{r, echo=TRUE,eval=TRUE}

lmFit_forward
tuned_perf <- caret::postResample(pred = bh_pred_forward, obs = data_test$medv)
tuned_perf

```

### Step 3: Fitting our own ridgereg() Ridge Regression function for various values of $\lambda$

We use an approach where we loop over different values $\lambda$ and then fit our ridgereg() function to the training data and then predict values of the test data.

```{r, echo=TRUE, eval=TRUE}
lambda = seq(0, 25, by = 1)
y_rmse <- c()
c_rsq <- c()
c_mae <- c()
for(i in lambda){
  x <- ridgereg$new(formula = medv~., data = data_train, lambda = i)
  x_pred <- x$predict(data_test)
  tuned_perf_ridge <- postResample(pred = x_pred, obs = data_test$medv)
  y_rmse <- c(y_rmse, tuned_perf_ridge[1])
  c_rsq <- c(c_rsq, tuned_perf_ridge[2])
  c_mae <- c(c_mae, tuned_perf_ridge[3])
}
tuned_ridge <- data.frame(lambda = lambda,
                          RMSE = y_rmse,
                          RSQ = c_rsq,
                          MAE = c_mae)
tuned_perf_ridge
```

### Step 4: Finding best hyperparameter value for $\lambda$ by 10-fold cross-validation.

```{r, eval=TRUE, echo=TRUE}
ctrl <- trainControl(method = 'repeatedcv', repeats = 10)
ridgefit <- caret::train(medv~.,
                  data = data_train,
                  method = 'ridge',
                  preProcess = c('scale', 'center'),
                  tuneLength = 10,
                  trControl = ctrl
                  )
ridgefit
plot(ridgefit)

```

### Step 5: Comparision performance of above models on test dataset:
We can compare the performance of various models by comparing RMSE of each models.

#### Performance evaluation of Linear Regression model fit for predicting test data

```{r, echo=TRUE, eval=TRUE}
tuned_perf <- caret::postResample(pred = bh_pred_forward, obs = data_test$medv)
tuned_perf
```

#### Performance evaluation of Ridge Regression model fit for predicting test data
```{r, echo=TRUE, eval=TRUE}
head(tuned_ridge[order(tuned_ridge$RMSE),],5)
```

#### Performance evaluation of Ridge Regression by 10-fold-cross validation

```{r, echo=TRUE, eval=TRUE}
ridge_pred <- predict(ridgefit, data_test)
ridge_perf_rmse <- postResample(pred = ridge_pred, obs = data_test$medv)
ridge_perf_rmse
```

As seen above, the model fitted with linear regression with forward selection of covariates is giving the least RMSE. This is followed by the 10-fold cross-validation and the ridgereg function developed by us.